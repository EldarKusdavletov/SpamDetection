import pandas as pd
import numpy as np
import pickle
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping

# --- –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã ---
VOCAB_SIZE = 5000
MAX_LEN = 150
EMBEDDING_DIM = 128
LSTM_UNITS = 64
NUM_EPOCHS = 10
BATCH_SIZE = 32
MODEL_PATH = 'spam_classifier_tensorflow.keras'
TOKENIZER_PATH = 'tokenizer_tensorflow.pickle'
DATASET_PATH = "hf://datasets/darkQibit/russian-spam-detection/processed_combined.parquet"


# --- 1. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö ---

def load_data(path: str) -> pd.DataFrame:
    try:
        print("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
        return pd.read_parquet(path)
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
        return pd.DataFrame()


def preprocess_data(df: pd.DataFrame):
    X_train, X_test, y_train, y_test = train_test_split(df['message'], df['label'], test_size=0.2, random_state=42)

    tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token="<unk>")
    tokenizer.fit_on_texts(X_train)

    train_sequences = tokenizer.texts_to_sequences(X_train)
    test_sequences = tokenizer.texts_to_sequences(X_test)

    train_padded = pad_sequences(train_sequences, maxlen=MAX_LEN, padding='post', truncating='post')
    test_padded = pad_sequences(test_sequences, maxlen=MAX_LEN, padding='post', truncating='post')

    return train_padded, test_padded, y_train.values, y_test.values, tokenizer


# --- 2. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ ---

def build_model():
    model = Sequential([
        Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM),
        LSTM(LSTM_UNITS, dropout=0.2, recurrent_dropout=0.2),
        Dropout(0.5),
        Dense(1, activation='sigmoid')
    ])

    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model


# --- –û—Å–Ω–æ–≤–Ω–æ–π —Å–∫—Ä–∏–ø—Ç ---
if __name__ == "__main__":
    df = load_data(DATASET_PATH)

    X_train, X_test, y_train, y_test, tokenizer = preprocess_data(df)

    print("–°–æ–∑–¥–∞–Ω–∏–µ –∏ –∫–æ–º–ø–∏–ª—è—Ü–∏—è –º–æ–¥–µ–ª–∏...")
    model = build_model()
    model.summary()

    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

    print("–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è...")
    history = model.fit(
        X_train, y_train,
        epochs=NUM_EPOCHS,
        batch_size=BATCH_SIZE,
        validation_data=(X_test, y_test),
        callbacks=[early_stopping],
        verbose=1
    )

    print("\n–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö:")
    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)
    print(f"–ü–æ—Ç–µ—Ä–∏: {loss:.4f} | –¢–æ—á–Ω–æ—Å—Ç—å: {accuracy * 100:.2f}%")

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
    model.save(MODEL_PATH)
    print(f"\nüíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {MODEL_PATH}")

    with open(TOKENIZER_PATH, 'wb') as f:
        pickle.dump(tokenizer, f)
    print(f"üíæ –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {TOKENIZER_PATH}")
